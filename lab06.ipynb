{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b54a80e-1eab-44f7-9e4d-775efa992fc8",
   "metadata": {},
   "source": [
    "# Lab 6: Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715d260-2854-4680-95ce-6aa1c6ee2ee9",
   "metadata": {},
   "source": [
    "## High-level steps for training a model with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fb34af9-cf4e-4633-93c1-c764c5afa16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a65ff14-96cb-423a-8d8c-6a65270c1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP I: Setup sequential layers of the network\n",
    "# This code sets up the entire architecture of the network.\n",
    "\n",
    "model = Sequential(\n",
    "    [               \n",
    "        # tf.keras.Input(shape=(400,)),    #specify input size\n",
    "\n",
    "        # We do NOT have to specify input_dim within each layer because we already specified the original input size (the input to layer1) in the line above.\n",
    "        # The inputs to the subsequent layers are known... they will be the number of units from the previous layer\n",
    "        Dense(25, activation='sigmoid', name='layer1'),\n",
    "        Dense(15, activation='sigmoid', name='layer2'),\n",
    "        Dense(1, activation='sigmoid', name='layer3')\n",
    "        \n",
    "    ], \n",
    "    # name = \"my_model\" \n",
    ") \n",
    "\n",
    "# This step aligns with the development of the basic (non-optimized) function of the line of best fit. Each line above specifies the cost function (aka activation).\n",
    "\n",
    "# If linear/polynomial regression:\n",
    "    # f_wb_i = np.dot(X[i], w) + b      ... analogous to y = mx + b\n",
    "\n",
    "# If logistic regression (as specified in the above code with 'sigmoid'):\n",
    "    # z = np.dot(w, x) + b              ... analogous to y = mx + b because we use linear regression within logistic regression\n",
    "    # f_x = 1/(1 + np.exp(-z))          ... this is the sigmoid function that we use for logistical regression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cf9b193-f501-460d-804c-d44d6e5a5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP II: Compile the model; must specify the loss function to use\n",
    "\n",
    "# model.compile(loss=MeanSquaredError()) # for linear/polynomial regression\n",
    "\n",
    "# OR\n",
    "\n",
    "model.compile(loss=BinaryCrossentropy()) # for logistic regression\n",
    "\n",
    "# This step aligns with the calculation of loss (difference between expected and actual) for each input.\n",
    "# This step also includes the averaging of that loss over the entire data set, which results in a cost function. \n",
    "\n",
    "# If linear/polynomial regression:\n",
    "    # cost = (f_wb_i - y[i])**2         ... 'squared error cost'; only appropriate for linear/polynomial regression (where the output is a continuous range of numbers) because it results in a convex function with one minimum.\n",
    "\n",
    "# If logistic regression (as specified above by BinaryCrossentropy):\n",
    "    #  cost =  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)    \n",
    "    # ... Squared Error cost function is not appropriate for logistic regression because there would be multiple minimums.\n",
    "    # BinaryCrossentropy loss function is just another name for the logistic loss function; 'binary' because the final output is one of two values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dbd6fe1-a101-481c-9b83-d00f7880e3d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# STEP III: Fit the model (that is defined as model in the first step) using the loss function (specified in the compile call above) to the dataset (X, Y)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# epochs is the number of steps to take during gradient descent (if doing batch gradient descent)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m.fit(X, Y, epochs=\u001b[32m100\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# This step aligns with performing gradient descent (adjusting weights and bias at each step of gradient descent. \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Tensorflow uses \"back propogation\" to perform gradient descent. And it can even use different approaches that are more efficient than gradient descent.\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# STEP III: Fit the model (that is defined as model in the first step) using the loss function (specified in the compile call above) to the dataset (X, Y)\n",
    "# epochs is the number of steps to take during gradient descent (if doing batch gradient descent)\n",
    "model.fit(X, Y, epochs=100)\n",
    "\n",
    "# This step aligns with performing gradient descent (adjusting weights and bias at each step of gradient descent. \n",
    "# Tensorflow uses \"back propogation\" to perform gradient descent. And it can even use different approaches that are more efficient than gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5acb48-6af1-49fd-8f48-cc507ad8acc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9935a03-9f13-45c4-a33b-a8b5026f0ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
