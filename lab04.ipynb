{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a57cfd-1ec7-4a4c-94a5-79ce2c937db5",
   "metadata": {},
   "source": [
    "# Lab 4: Forward propagation with vectorized implementation\n",
    "\n",
    "Vectorized implementations use matrix math that executes operations in parallel. This is why vectorized implementations are more efficient than non-vectorized implementations, which execute operations sequentially (e.g. using a for loop). An additional boost to efficiency is achieved when using a GPU to process vectorized implementations because GPUs are particularly good at performing vectorized calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebe35ad-9dc7-4f22-8b6d-3c3599319c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "from lib.lab_utils_common import dlc, sigmoid\n",
    "# from lib.lab_coffee_utils import load_coffee_data, plt_roast, plt_prob, plt_layer, plt_network, plt_output_unit\n",
    "\n",
    "# import logging\n",
    "# logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "# tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc8f846-2135-4474-ad06-fd4962ed1db1",
   "metadata": {},
   "source": [
    "## Non-vectorized implementation (from previous labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e0619-9171-40de-aa99-9537f97544ec",
   "metadata": {},
   "source": [
    "### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0c8c658-c82c-4cf8-97e5-f73c885f2f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input values (1 training example with 2 features)\n",
    "x = np.array([200, 17]) # 1D array (non-vectorized)\n",
    "\n",
    "# weights for layer 1: \n",
    "# 2 input features per unit = 2 rows \n",
    "# 3 units in the layer = 3 cols\n",
    "weights1 = np.array([[1, -3, 5], \n",
    "                [-2, 4, -6]]) \n",
    "\n",
    "# biases for layer 1:\n",
    "# always 1 bias per unit = 1 row\n",
    "# 3 units in the layer = 3 cols\n",
    "biases1 = np.array([-1, 1, 2]) # 1D array (non-vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163890d-ab59-4dd3-9e99-cec0845dc257",
   "metadata": {},
   "source": [
    "### Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e4bfada-e518-43fb-9ce7-87e54e620b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the activation function g\n",
    "# This should be a sigmoid function because we are doing classification.\n",
    "# sigmoid is implemented in lab_utils_common; we simply rename the function here\n",
    "g = sigmoid\n",
    "\n",
    "# for a single layer, combine functions and activations\n",
    "# W is a matrix of weights for this layer; rows = features for this layers (number of outputs from previous layer); cols = units for this layer\n",
    "def my_dense(a_in, W, b):\n",
    "    # print(\"\\nentering my dense with variables:\")\n",
    "    # print(a_in) # This has two rows instead of 1!!! \n",
    "    # print(W)\n",
    "    # print(b)\n",
    "\n",
    "    unit_count = W.shape[1]\n",
    "    \n",
    "    # make dummy array to accumulate results\n",
    "    a_out = np.zeros(unit_count)\n",
    "\n",
    "    # each loop represents one unit/function of the layer\n",
    "    for j in range(unit_count):\n",
    "        # print(f\"on unit {j}\")\n",
    "        \n",
    "        weights_for_this_unit = W[:,j] # accesses the jth column of the weights matrix... the weights for the unit j\n",
    "        \n",
    "        z = np.dot( weights_for_this_unit, a_in) + b[j]  # analogous to f(x) = wx + b\n",
    "        \n",
    "        a_out[j] = g(z) # sigmoid because doing classification; NB: you could pass g in as a parameter to this function\n",
    "        # print(a_out)\n",
    "    \n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51344f3a-c3b5-43cd-bd67-d867bc5a16ca",
   "metadata": {},
   "source": [
    "## Vectorized implementation\n",
    "\n",
    "A major difference between the vectorized and non-vectorized versions is:\n",
    "* non-vectorized: Use 1D array when only 1 row of data.\n",
    "* vectorized: Use 2D array regardless of how many rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c9e5c-e040-43e3-ba56-8b97c58d2c37",
   "metadata": {},
   "source": [
    "### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f39af2b5-677a-45ab-9017-6a0c001278ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input values (1 training example with 2 features)\n",
    "XT = np.array([[200, 17]]) # 2D array (vectorized); T denotes that this is a transposed matrix but we manually code the original input in a transposed way\n",
    "\n",
    "# weights for layer 1: \n",
    "# 2 input features per unit = 2 rows \n",
    "# 3 units in the layer = 3 cols\n",
    "weights1 = np.array([[1, -3, 5], \n",
    "                [-2, 4, -6]])          # same 2D array for vectorized and non-vectorized approaches\n",
    "\n",
    "# biases for layer 1:\n",
    "# always 1 bias per unit = 1 row\n",
    "# 3 units in the layer = 3 cols\n",
    "biases1 = np.array([[-1, 1, 2]]) # 2D array (vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d468c-3d2e-411f-b4b8-40b178732911",
   "metadata": {},
   "source": [
    "### Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9334c34c-b56c-4d1b-b872-157c90fecb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z values in this layer: [[ 165 -531  900]]\n",
      "output from this layer: [[1.00e+000 7.12e-218 1.00e+000]]\n"
     ]
    }
   ],
   "source": [
    "# use capital letters to represent matrices; lowercase to represent vectors and scalars\n",
    "# ... in a vectorized approach almost everything will be a matrix so we can do parallelized matrix math\n",
    "def my_dense_vectorized(A_in, W, B):\n",
    "\n",
    "    # no for-loop required in the vectorized implementation because the calculations are done in parallel (not synchronously)\n",
    "    # This means that all units/functions of the layer are calculated simultaneously\n",
    "    \n",
    "    Z = np.matmul(A_in, W) + B  # matmul is a NumPy function that does matrix multiplication \n",
    "    print(f\"Z values in this layer: {Z}\")\n",
    "    \n",
    "    A_out = g(Z) # function g applied element-wise\n",
    "\n",
    "    return A_out\n",
    "\n",
    "\n",
    "result = my_dense_vectorized(X, weights1, biases1)\n",
    "print(f\"output from this layer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9b19d-6a79-4ab2-9b18-7172f14b5edf",
   "metadata": {},
   "source": [
    "## Matrix Math\n",
    "\n",
    "This section is not linked to the examples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6ca00bc-52d2-44ae-840f-5654c4579812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   2. ]\n",
      " [-1.  -2. ]\n",
      " [ 0.1  0.2]]\n"
     ]
    }
   ],
   "source": [
    "# matrix of input values (could be activation values from a previous layer)\n",
    "A = np.array([[1, -1, 0.1],\n",
    "             [2, -2, 0.2]])\n",
    "\n",
    "# We must transpose A in order to do a dot product calculation with W.\n",
    "\n",
    "\n",
    "# manual transpose\n",
    "# AT = np.array([\n",
    "#     [1, 2],\n",
    "#     [-1, -2],\n",
    "#     [0.1, 0.2],\n",
    "# ])\n",
    "\n",
    "# auto transpose\n",
    "AT = A.T\n",
    "\n",
    "print(AT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a090b50e-bb65-4652-83bd-02a9d5755526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 5 7 9]\n",
      " [4 6 8 0]]\n"
     ]
    }
   ],
   "source": [
    "# matrix of weights for one layer\n",
    "W = np.array([[3, 5, 7, 9],\n",
    "             [4, 6, 8, 0]])\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56faeba2-e4c6-4667-adb2-56fc3ac9e78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   17.   23.    9. ]\n",
      " [-11.  -17.  -23.   -9. ]\n",
      " [  1.1   1.7   2.3   0.9]]\n"
     ]
    }
   ],
   "source": [
    "# Use matmul to calculate dot product of AT and W (parallel processing)\n",
    "\n",
    "Z = np.matmul(AT, W)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2e650-2967-4918-9ccc-3591f54ae4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5db25e-ad4b-44f7-a333-7d02edc3c177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
